{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c328de-336b-4bbd-8fd9-96ac3de78aa1",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2c5bc7b-3309-4b3f-b9ea-14ea796629ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service  \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe8ad7-ce49-49b0-812c-aa23896614dc",
   "metadata": {},
   "source": [
    "##  Step2: Scrape the Key Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead514da-4d3f-4eff-8abd-3c639d42c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def scrape_wikipedia_page(url, filename):\n",
    "    \"\"\"\n",
    "    Robust Wikipedia scraper that handles different page structures\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set headers to mimic a real browser\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Scraping page: {url}\")\n",
    "        \n",
    "        # Send GET request with headers\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Try multiple methods to find content\n",
    "        content_text = \"\"\n",
    "        \n",
    "        # Method 1: Try to find the main content div\n",
    "        main_content = soup.find('div', {'class': 'mw-parser-output'})\n",
    "        if main_content:\n",
    "            logger.info(\"Found content using mw-parser-output\")\n",
    "            content_text = main_content.get_text()\n",
    "        \n",
    "        # Method 2: If first method fails, try getting all paragraphs\n",
    "        elif not content_text:\n",
    "            logger.info(\"Trying alternative method - getting all paragraphs\")\n",
    "            paragraphs = soup.find_all('p')\n",
    "            for p in paragraphs:\n",
    "                text = p.get_text().strip()\n",
    "                if text and len(text) > 20:  # Filter out short paragraphs\n",
    "                    content_text += text + '\\n\\n'\n",
    "        \n",
    "        # Method 3: If still no content, get body text\n",
    "        if not content_text.strip():\n",
    "            logger.info(\"Using body text as fallback\")\n",
    "            body = soup.find('body')\n",
    "            if body:\n",
    "                content_text = body.get_text()\n",
    "        \n",
    "        if content_text.strip():\n",
    "            # Clean the text\n",
    "            lines = []\n",
    "            for line in content_text.split('\\n'):\n",
    "                cleaned_line = line.strip()\n",
    "                if (cleaned_line and \n",
    "                    len(cleaned_line) > 10 and  # Minimum length\n",
    "                    not cleaned_line.startswith('Jump to') and\n",
    "                    not cleaned_line.startswith('Navigation menu') and\n",
    "                    not cleaned_line.startswith('Main page') and\n",
    "                    'cookie' not in cleaned_line.lower() and\n",
    "                    'javascript' not in cleaned_line.lower()):\n",
    "                    lines.append(cleaned_line)\n",
    "            \n",
    "            cleaned_text = '\\n'.join(lines)\n",
    "            \n",
    "            # Add page title at the top\n",
    "            title = soup.find('h1')\n",
    "            if title:\n",
    "                page_title = title.get_text().strip()\n",
    "                cleaned_text = f\"# {page_title}\\n\\n\" + cleaned_text\n",
    "            \n",
    "            # Save to file\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(cleaned_text)\n",
    "            \n",
    "            logger.info(f\"Successfully saved content to {filename}\")\n",
    "            logger.info(f\"Total characters: {len(cleaned_text)}\")\n",
    "            logger.info(f\"Total lines: {len(lines)}\")\n",
    "            \n",
    "            return cleaned_text, len(cleaned_text), len(lines)\n",
    "        else:\n",
    "            logger.error(\"No content could be extracted from the page\")\n",
    "            return None, 0, 0\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        return None, 0, 0\n",
    "\n",
    "# Scrape the Key Events of the 20th century page\n",
    "url = \"https://en.wikipedia.org/wiki/Key_events_of_the_20th_century\"\n",
    "output_file = \"key_events_20th_century.txt\"\n",
    "\n",
    "content, char_count, line_count = scrape_wikipedia_page(url, output_file)\n",
    "\n",
    "if content:\n",
    "    print(\"Scraping completed successfully!\")\n",
    "    print(f\"File saved: {output_file}\")\n",
    "    print(f\"Total characters: {char_count}\")\n",
    "    print(f\"Total lines: {line_count}\")\n",
    "    \n",
    "    # Display a preview\n",
    "    print(\"\\n--- PREVIEW OF CONTENT ---\")\n",
    "    lines = content.split('\\n')\n",
    "    for i, line in enumerate(lines[:15]):\n",
    "        if line.strip():\n",
    "            print(f\"{line[:120]}{'...' if len(line) > 120 else ''}\")\n",
    "else:\n",
    "    print(\"Scraping failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e6f0f-fbe1-4ab7-8beb-31f07702a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the created file\n",
    "import os\n",
    "\n",
    "def verify_file():\n",
    "    filename = 'key_events_20th_century.txt'\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        print(\"File verification successful!\")\n",
    "        print(f\"File location: {os.path.abspath(filename)}\")\n",
    "        print(f\"File size: {len(content)} characters\")\n",
    "        print(f\"File size: {os.path.getsize(filename)} bytes\")\n",
    "        \n",
    "        # Show file structure\n",
    "        lines = content.split('\\n')\n",
    "        print(f\"Total lines: {len(lines)}\")\n",
    "        \n",
    "        print(\"\\n--- FILE STRUCTURE ---\")\n",
    "        for i, line in enumerate(lines[:20]):  # Show first 20 lines\n",
    "            if line.strip():\n",
    "                print(f\"Line {i+1}: {line[:80]}{'...' if len(line) > 80 else ''}\")\n",
    "                \n",
    "        return True\n",
    "    else:\n",
    "        print(\"File not found\")\n",
    "        return False\n",
    "\n",
    "verify_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66381e7-9f9b-46eb-b511-08f0aeae05dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
